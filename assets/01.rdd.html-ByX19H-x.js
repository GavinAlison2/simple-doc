import{_ as a,c as n,e as t,o as e}from"./app-DzmgiGLk.js";const p="/simple-doc/assets/1-CQtralVG.png",l={};function i(o,s){return e(),n("div",null,s[0]||(s[0]=[t(`<h1 id="rdd" tabindex="-1"><a class="header-anchor" href="#rdd"><span>RDD</span></a></h1><p>Spark使用称为RDD(弹性分布式数据集)的专用基础数据结构，该结构是跨机器分区的逻辑数据集合。 RDD可以通过两种方式创建：</p><ul><li>一种是通过引用外部存储系统中的数据集，</li><li>第二种是通过对现有RDD进行转换(例如，映射，过滤器，化简，联接)。</li></ul><p>RDD抽象是通过语言集成的API公开的。</p><p>特性：</p><ul><li><p>Resilient（弹性）：RDD之间会形成有向无环图（DAG），如果RDD丢失了或者失效了，可以从父RDD重新计算得到。即容错性。</p></li><li><p>Distributed（分布式）：RDD的数据是以逻辑分区的形式分布在集群的不同节点的。</p></li><li><p>Dataset（数据集）：即RDD存储的数据记录，可以从外部数据生成RDD，例如Json文件，CSV文件，文本文件，数据库等。</p></li><li><p>内存计算: RDD可以被缓存到内存中，以便在本地进行快速计算。</p></li><li><p>并行计算: RDD可以并行计算，以便利用集群的多核CPU。</p></li><li><p>惰性求值: RDD的操作不会立即执行，而是等到需要结果的时候才执行。</p></li><li><p>容错性: RDD可以容忍节点失效和数据丢失。</p></li><li><p>不变性: RDD的操作不会改变RDD的结构和内容。</p></li><li><p>持久化: RDD可以持久化到内存或磁盘，以便在集群的不同节点上进行计算。</p></li></ul><h2 id="_1-创建rdd" tabindex="-1"><a class="header-anchor" href="#_1-创建rdd"><span>1. 创建RDD</span></a></h2><h3 id="_1-1-引用外部存储系统中的数据集" tabindex="-1"><a class="header-anchor" href="#_1-1-引用外部存储系统中的数据集"><span>1.1 引用外部存储系统中的数据集</span></a></h3><p>Spark可以从各种外部存储系统（如Hadoop，HBase，Cassandra，Kafka等）中读取数据集并创建RDD。</p><ul><li>存储系统的数据集， local, hdfs, hbase, cassandra, kafka, mongoDB, redis等。</li><li>读取方式： <ul><li>文本文件：sc.textFile(&quot;file:///path/to/file&quot;)</li><li>压缩文件：sc.textFile(&quot;file:///path/to/file.gz&quot;)</li><li>目录：sc.wholeTextFiles(&quot;file:///path/to/dir&quot;)</li><li>数据库： <ul><li>JDBC：sc.jdbc(&quot;jdbc:mysql://localhost/test&quot;, &quot;table&quot;, &quot;columns&quot;)</li><li>Hive：sc.sql(&quot;select * from table&quot;)</li><li>Presto：sc.presto(&quot;select * from table&quot;)</li><li>Elasticsearch：sc.es(&quot;http://localhost:9200/index/type&quot;)</li><li>其他：sc.newAPIHadoopFile(&quot;file:///path/to/file&quot;, &quot;inputformat&quot;, &quot;keyclass&quot;, &quot;valueclass&quot;, conf)</li></ul></li></ul></li><li>Scala 集合创建： <ul><li>键值对：sc.parallelize(Seq((1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;)))</li><li>序列：sc.parallelize(Seq(1, 2, 3))</li><li>集合：sc.parallelize(Set(1, 2, 3))</li><li>数组：sc.parallelize(Array(1, 2, 3))</li><li>元组：sc.parallelize(Seq((1, &quot;a&quot;), (2, &quot;b&quot;), (3, &quot;c&quot;)))</li><li>字典：sc.parallelize(Map(1 -&gt; &quot;a&quot;, 2 -&gt; &quot;b&quot;, 3 -&gt; &quot;c&quot;))</li><li>其他：sc.emptyRDD()</li></ul></li><li>其他RDD转换： <ul><li>键值对：rdd.map(t =&gt; (t._2, t._1))</li><li>flatMap：rdd.flatMap(t =&gt; t.split(&quot; &quot;))</li><li>makeRDD：sc.makeRDD(List(1,2,3,4,5,6,7,8))</li></ul></li></ul><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token comment">// 从Hadoop文件系统中读取数据集</span></span>
<span class="line"><span class="token keyword">val</span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;hdfs://path/to/file&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span class="token comment">// 从HBase表中读取数据集</span></span>
<span class="line"><span class="token keyword">val</span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>newAPIHadoopRDD<span class="token punctuation">(</span></span>
<span class="line">    <span class="token string">&quot;org.apache.hadoop.hbase.mapreduce.TableInputFormat&quot;</span><span class="token punctuation">,</span></span>
<span class="line">    <span class="token string">&quot;org.apache.hadoop.hbase.io.ImmutableBytesWritable&quot;</span><span class="token punctuation">,</span></span>
<span class="line">    <span class="token string">&quot;org.apache.hadoop.hbase.client.Result&quot;</span><span class="token punctuation">,</span></span>
<span class="line">    conf<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="_1-2-对现有rdd进行转换" tabindex="-1"><a class="header-anchor" href="#_1-2-对现有rdd进行转换"><span>1.2 对现有RDD进行转换</span></a></h3><p>RDD 支持两种操作：</p><ul><li>转换操作（Transformation）</li><li>行动操作（Actions）</li></ul><h4 id="_1-2-1-转换操作" tabindex="-1"><a class="header-anchor" href="#_1-2-1-转换操作"><span>1.2.1 转换操作：</span></a></h4><p>转换操作以RDD做为输入参数，然后输出一个或者多个RDD。转换操作不会修改输入RDD。Map()、Filter()这些都属于转换操作。</p><p>转换操作是惰性求值操作，只有在碰到行动操作（Actions）的时候，转换操作才会真正实行。转换操作分两种：窄依赖和宽依赖（上文提到过）。 窄依赖， 即输入RDD的每个分区只依赖于一个分区(前RDD)的输出结果，如map()操作。 1v1 宽依赖， 即输入RDD的每个分区依赖于整个(多个前)RDD的输出结果，如reduce()操作。 Nv1</p><ul><li><p>map：对每个元素进行映射操作, 结果仍然是RDD。</p></li><li><p>mapPartitions：对每个分区进行映射操作，func在类型T的RDD上运行时必须为<code>Iterator &lt;T&gt;⇒Iterator &lt;U&gt;</code>类型。</p></li><li><p>mapPartitionsWithIndex：对每个分区进行映射操作，当在类型T的RDD上运行时，func的类型必须为<code>(Int，Iterator &lt;T&gt;)⇒Iterator &lt;U&gt;</code>。</p></li><li><p>mapValues：对每个键值对的value进行映射操作，结果仍然是RDD。</p></li><li><p>filter：过滤出满足条件的元素。</p></li><li><p>flatMap：对每个元素进行映射操作, 每个输入项都可以映射到0个或多个输出项(因此func应该返回Seq而不是单个项)。</p></li><li><p>flatMapValues：对每个键值对的value进行映射操作，然后将结果展开。</p></li><li><p>sample：随机抽样, 结果仍然是RDD。</p></li><li><p>union(otherDataSet)：将两个RDD合并, 结果仍然是RDD。</p></li><li><p>intersection(otherDataSet)：求两个RDD的交集。</p></li><li><p>subtract(otherDataSet)：求两个RDD的差集。</p></li><li><p>distinct：去除重复元素。</p></li><li><p>groupBy：将元素分组。</p></li><li><p>groupByKey：将元素按键值对分组, 在(K，V)对的数据集上调用时，返回<code>(K，Iterable &lt;V&gt;)</code>对的数据集,注–如果要分组以便对每个键执行聚合(例如求和或平均值)，则使用reduceByKey或AggregateByKey将产生更好的性能。</p></li><li><p>reduceByKey(func, [numTasks])：对每个键值对进行聚合操作, 在(K，V)对的数据集上调用时，返回(K，V)对的数据集，其中每个键的值使用给定的reduce函数func进行汇总，该函数必须为(V，V)⇒V类型与groupByKey中一样，reduce任务的数量可以通过可选的第二个参数进行配置。</p></li><li><p>aggregateByKey(zeroValue, seqOp, combOp)：与reduceByKey类似，但允许用户提供一个初始值，该初始值将与每个键的初始值一起使用，并与每个分区的元素一起聚合。在(K，V)对的数据集上调用时，返回(K，U)对的数据集，其中每个键的值使用给定的Combine函数和中性的“零”值进行汇总。允许与输入值类型不同的聚合值类型，同时避免不必要的分配。像groupByKey中一样，reduce任务的数量可以通过可选的第二个参数进行配置。</p></li><li><p>sortByKey：根据键值对进行排序。在由K实现Ordered的(K，V)对的数据集上调用时，返回(K，V)对的数据集，按布尔值升序参数指定按键升序或降序排序。</p></li><li><p>sort：排序。</p></li><li><p>join(otherDataSet, [numTasks])：将两个RDD进行连接操作, 结果仍然是RDD。在(K，V)和(K，W)类型的数据集上调用时，返回(K，(V，W))对的数据集，其中每个键都有所有成对的元素。通过leftOuterJoin，rightOuterJoin和fullOuterJoin支持外部联接。</p></li><li><p>cogroup(otherDataSet, [numTasks])：将两个RDD进行合并分组操作, 结果仍然是RDD。在(K，V)和(K，W)类型的数据集上调用时，返回<code>(K，(Iterable &lt;V&gt;，Iterable &lt;W&gt;))</code>对的数据集，其中每个键都有所有元素。</p></li><li><p>cartesian：笛卡尔积。在类型T和U的数据集上调用时，返回(T，U)对(所有元素对)的数据集。</p></li><li><p>pipe(command, [envVars])：在RDD上执行外部命令。通过shell命令通过管道传输RDD的每个分区Perl或bash脚本。将RDD元素写入进程的stdin，并将输出到其stdout的行作为字符串的RDD返回。</p></li><li><p>coalesce(numPartitions)：合并分区。将RDD中的分区数减少到numPartitions.</p></li><li><p>repartition(numPartitions)：重新分区。将RDD中的分区数重新设置为numPartitions。保持分区平衡.</p></li><li><p>repartitionAndSortWithinPartitions(partitioner, [ascending])：重新分区并排序。与repartition类似，但在每个分区内进行排序。</p></li></ul><h4 id="_1-2-2-行动操作" tabindex="-1"><a class="header-anchor" href="#_1-2-2-行动操作"><span>1.2.2 行动操作：</span></a></h4><ul><li><p>reduce(func)：对元素进行聚合操作。</p></li><li><p>collect：将RDD中的元素收集到Driver程序中,返回数组。</p></li><li><p>count：计算元素数量。</p></li><li><p>first：返回第一个元素。</p></li><li><p>take(n)：返回前n个元素。</p></li><li><p>takeSample(withReplacement, num, [seed])：随机抽样。</p></li><li><p>takeOrdered(n, [ordering])：返回前n个元素，按排序顺序。</p></li><li><p>saveAsTextFile(path)：将RDD中的元素保存到文本文件中。写入本地文件系统，HDFS或任何其他Hadoop支持的文件系统中的给定目录中。 Spark在每个元素上调用toString将其转换为文件中的一行文本。</p></li><li><p>saveAsSequenceFile(path)：将RDD中的元素保存到SequenceFile中。</p></li><li><p>saveAsObjectFile(path)：将RDD中的元素保存到对象文件中。</p></li><li><p>countByKey：计算每个键的元素数量。仅在类型(K，V)的RDD上可用。返回(K，Int)对的哈希图以及每个键的计数.</p></li><li><p>foreach(func)：对每个元素进行操作。</p></li><li><p>foreachPartition(func)：对每个分区进行操作。</p></li><li><p>foreachPartitionWithIndex(func)：对每个分区进行操作，同时传入分区索引。</p></li></ul><h2 id="例子-wordcount" tabindex="-1"><a class="header-anchor" href="#例子-wordcount"><span>例子，WordCount</span></a></h2><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code><span class="line"><span class="token function">docker</span> run <span class="token parameter variable">-ti</span> <span class="token parameter variable">--rm</span> <span class="token parameter variable">--name</span> spark <span class="token parameter variable">-p</span> <span class="token number">4040</span>:4040 <span class="token parameter variable">-p</span> <span class="token number">8080</span>:8080 <span class="token parameter variable">-p</span> <span class="token number">7077</span>:7077 <span class="token parameter variable">-v</span> ./data:/opt/spark/work-dir/data spark:3.5.5  <span class="token function">bash</span> <span class="token parameter variable">-c</span> <span class="token string">&quot;../bin/spark-shell&quot;</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>data/input.txt</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">people are not as beautiful as they look, </span>
<span class="line">as they walk or as they talk.</span>
<span class="line">they are only as beautiful  as they love, </span>
<span class="line">as they care as they share.</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token punctuation">.</span>/bin<span class="token operator">/</span>spark<span class="token operator">-</span>shell</span>
<span class="line"></span>
<span class="line">scala<span class="token operator">&gt;</span> <span class="token keyword">val</span> input <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;input.txt&quot;</span><span class="token punctuation">)</span></span>
<span class="line">input<span class="token operator">:</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>rdd<span class="token punctuation">.</span>RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> input<span class="token punctuation">.</span>txt MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span> at textFile at <span class="token generics"><span class="token punctuation">&lt;</span>console<span class="token punctuation">&gt;</span></span><span class="token operator">:</span><span class="token number">24</span></span>
<span class="line"></span>
<span class="line">scala<span class="token operator">&gt;</span> <span class="token keyword">val</span> words <span class="token operator">=</span> input<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot; &quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">words<span class="token operator">:</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>rdd<span class="token punctuation">.</span>RDD<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> at flatMap at <span class="token generics"><span class="token punctuation">&lt;</span>console<span class="token punctuation">&gt;</span></span><span class="token operator">:</span><span class="token number">25</span></span>
<span class="line"></span>
<span class="line">scala<span class="token operator">&gt;</span> <span class="token keyword">val</span> pairs <span class="token operator">=</span> words<span class="token punctuation">.</span>map<span class="token punctuation">(</span> <span class="token punctuation">(</span>_<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line">pairs<span class="token operator">:</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>rdd<span class="token punctuation">.</span>RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> MapPartitionsRDD<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span> at map at <span class="token generics"><span class="token punctuation">&lt;</span>console<span class="token punctuation">&gt;</span></span><span class="token operator">:</span><span class="token number">26</span></span>
<span class="line"></span>
<span class="line">scala<span class="token operator">&gt;</span> <span class="token keyword">val</span> counts <span class="token operator">=</span> pairs<span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span></span>
<span class="line">counts<span class="token operator">:</span> org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>rdd<span class="token punctuation">.</span>RDD<span class="token punctuation">[</span><span class="token punctuation">(</span><span class="token builtin">String</span><span class="token punctuation">,</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token punctuation">]</span> <span class="token operator">=</span> ShuffledRDD<span class="token punctuation">[</span><span class="token number">4</span><span class="token punctuation">]</span> at reduceByKey at <span class="token generics"><span class="token punctuation">&lt;</span>console<span class="token punctuation">&gt;</span></span><span class="token operator">:</span><span class="token number">27</span></span>
<span class="line"></span>
<span class="line">scala<span class="token operator">&gt;</span> counts<span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span><span class="token string">&quot;output&quot;</span><span class="token punctuation">)</span><span class="token comment">//触发action操作,才会去执行持久化</span></span>
<span class="line"></span>
<span class="line">scala<span class="token operator">&gt;</span> sc<span class="token punctuation">.</span>stop<span class="token punctuation">(</span><span class="token punctuation">)</span>    </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line">sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;data/input.txt&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot; &quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span><span class="token punctuation">(</span>_<span class="token punctuation">,</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span><span class="token punctuation">.</span>saveAsTextFile<span class="token punctuation">(</span><span class="token string">&quot;output.txt&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><p>检查输出：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">ls -al output/</span>
<span class="line">part-00000 </span>
<span class="line">part-00001 </span>
<span class="line">_SUCCESS</span>
<span class="line"></span>
<span class="line">cat output/part-00000 </span>
<span class="line">(people,1) </span>
<span class="line">(are,2) </span>
<span class="line">(not,1) </span>
<span class="line">(as,8) </span>
<span class="line">(beautiful,2) </span>
<span class="line">(they, 7) </span>
<span class="line">(look,1) </span>
<span class="line"></span>
<span class="line">cat output/part-00001 </span>
<span class="line">(walk, 1) </span>
<span class="line">(or, 1) </span>
<span class="line">(talk, 1) </span>
<span class="line">(only, 1) </span>
<span class="line">(love, 1) </span>
<span class="line">(care, 1) </span>
<span class="line">(share, 1) </span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>ui 查看</p><p>localhost:4040</p><p><img src="`+p+'" alt="result"></p>',31)]))}const u=a(l,[["render",i]]),r=JSON.parse('{"path":"/guide/etl/spark/01.rdd.html","title":"RDD","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"1. 创建RDD","slug":"_1-创建rdd","link":"#_1-创建rdd","children":[{"level":3,"title":"1.1 引用外部存储系统中的数据集","slug":"_1-1-引用外部存储系统中的数据集","link":"#_1-1-引用外部存储系统中的数据集","children":[]},{"level":3,"title":"1.2 对现有RDD进行转换","slug":"_1-2-对现有rdd进行转换","link":"#_1-2-对现有rdd进行转换","children":[]}]},{"level":2,"title":"例子，WordCount","slug":"例子-wordcount","link":"#例子-wordcount","children":[]}],"git":{"updatedTime":1744888563000,"contributors":[{"name":"alice","username":"alice","email":"921757697@qq.com","commits":1,"url":"https://github.com/alice"}],"changelog":[{"hash":"d0aa97b762c5a828ab8e3a7802989f2039337caf","time":1744888563000,"email":"921757697@qq.com","author":"alice","message":"deploy"}]},"filePathRelative":"guide/etl/spark/01.rdd.md"}');export{u as comp,r as data};
