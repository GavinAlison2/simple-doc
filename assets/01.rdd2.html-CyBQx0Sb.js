import{_ as n,c as a,d as e,o as p}from"./app-VCvVDv7C.js";const l={};function t(i,s){return p(),a("div",null,s[0]||(s[0]=[e(`<h1 id="rdd-编程指南" tabindex="-1"><a class="header-anchor" href="#rdd-编程指南"><span>RDD 编程指南</span></a></h1><h2 id="概述" tabindex="-1"><a class="header-anchor" href="#概述"><span>概述</span></a></h2><ul><li>rdd</li></ul><p>每个 Spark 应用程序都包含一个驱动程序，它运行用户的 main 函数并在集群上执行各种并行操作。 Spark 提供的主要抽象是弹性分布式数据集 (RDD)，它是一个分布在集群节点上的元素集合，可以并行操作。 创建于文件或者外部数据源、内存数据源，并对其进行转换。 用户还可以要求 Spark持久化一个 RDD 到内存中，使其能够在并行操作中有效地重复使用。 最后，RDD 会自动从节点故障中恢复。</p><ul><li>共享变量</li></ul><p>Spark 中的第二个抽象是共享变量,它可以在并行操作中使用。 默认情况下，当 Spark 并行运行一个函数作为一组在不同节点上的任务时，它会将函数中使用的每个变量的副本发送到每个任务。有时，需要在任务之间或任务和驱动程序之间共享一个变量。Spark 支持两种类型的共享变量：广播变量，它可以用来在所有节点的内存中缓存一个值，以及累加器，它是一些只能“添加”的变量，例如计数器和总和。</p><p>本指南展示了 Spark 支持的每种语言中的每个功能。如果你启动 Spark 的交互式 shell，最容易理解 - 对于 Scala shell，使用 bin/spark-shell.</p><h2 id="与-spark-链接" tabindex="-1"><a class="header-anchor" href="#与-spark-链接"><span>与 Spark 链接</span></a></h2><p>要编写 Spark 应用程序，你需要添加一个对 Spark 的 Maven 依赖项</p><div class="language-xml line-numbers-mode" data-highlighter="prismjs" data-ext="xml"><pre><code><span class="line"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span></span>
<span class="line">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.spark<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span></span>
<span class="line">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>spark-core_2.11<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span></span>
<span class="line">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.4.4<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span></span>
<span class="line"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>如果你想访问 HDFS 集群，你需要添加对你的 HDFS 版本的 hadoop-client 的依赖项</p><div class="language-xml line-numbers-mode" data-highlighter="prismjs" data-ext="xml"><pre><code><span class="line"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>dependency</span><span class="token punctuation">&gt;</span></span></span>
<span class="line">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>groupId</span><span class="token punctuation">&gt;</span></span>org.apache.hadoop<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>groupId</span><span class="token punctuation">&gt;</span></span></span>
<span class="line">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>artifactId</span><span class="token punctuation">&gt;</span></span>hadoop-client<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>artifactId</span><span class="token punctuation">&gt;</span></span></span>
<span class="line">    <span class="token tag"><span class="token tag"><span class="token punctuation">&lt;</span>version</span><span class="token punctuation">&gt;</span></span>2.7.3<span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>version</span><span class="token punctuation">&gt;</span></span></span>
<span class="line"><span class="token tag"><span class="token tag"><span class="token punctuation">&lt;/</span>dependency</span><span class="token punctuation">&gt;</span></span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="初始化-spark" tabindex="-1"><a class="header-anchor" href="#初始化-spark"><span>初始化 Spark</span></a></h2><p>然后，你需要创建一个 SparkConf 对象，它包含了 Spark 应用程序的配置信息。</p><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span></span><span class="token punctuation">{</span>SparkConf<span class="token punctuation">,</span> SparkContext<span class="token punctuation">}</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">val</span> conf <span class="token operator">=</span> <span class="token keyword">new</span> SparkConf<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setAppName<span class="token punctuation">(</span><span class="token string">&quot;MyApp&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>setMaster<span class="token punctuation">(</span><span class="token string">&quot;local&quot;</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> sc <span class="token operator">=</span> <span class="token keyword">new</span> SparkContext<span class="token punctuation">(</span>conf<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>SparkConf 类包含了许多配置选项，包括：</p><ul><li>setAppName(appName: String): 设置 Spark 应用程序的名称。</li><li>setMaster(master: String): 设置 Spark 集群的 URL。</li><li>set(key: String, value: String): 设置 SparkConf 中的其他配置选项。</li></ul><p>SparkContext 类是 Spark 应用程序的主要入口点，它负责创建 RDD、运行并行操作、管理持久化的变量以及从节点故障中恢复。</p><h2 id="使用-shell" tabindex="-1"><a class="header-anchor" href="#使用-shell"><span>使用 Shell</span></a></h2><p>Spark 提供了一个交互式 shell，你可以在其中运行 Spark 代码。 提供sc(SparkContext)变量，运行程序</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">$ ./bin/spark-shell</span>
<span class="line"></span>
<span class="line"># 在四核上运行</span>
<span class="line">$ ./bin/spark-shell --master local[4]</span>
<span class="line"></span>
<span class="line"># 连接到远程集群</span>
<span class="line">$ ./bin/spark-shell --master spark://master:7077</span>
<span class="line"></span>
<span class="line"># 连接到 YARN 集群</span>
<span class="line">$ ./bin/spark-shell --master yarn</span>
<span class="line"></span>
<span class="line"># 连接到 Mesos 集群</span>
<span class="line">$ ./bin/spark-shell --master mesos://master:5050</span>
<span class="line"></span>
<span class="line"># 连接到 Kubernetes 集群</span>
<span class="line">$ ./bin/spark-shell --master k8s://https://kubernetes.default.svc</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>其他的选项</p><ul><li>--jars: 加载额外的 jar 包。</li><li>--files: 上传到集群的本地文件。</li><li>--conf: 设置 SparkConf 中的配置选项。</li></ul><p>spark-submit 工具可以用来提交 Spark 应用程序到集群。 参数说明</p><ul><li>--class: 主类名。(例如 org.apache.spark.examples.SparkPi)</li><li>--master: Spark 集群的 URL。</li><li>--deploy-mode: 部署模式，可以是 &quot;client&quot; 或 &quot;cluster&quot;。</li><li>--jars: 加载额外的 jar 包。</li><li>--files: 上传到集群的本地文件。</li><li>--conf: 设置 SparkConf 中的配置选项。(例如 <code>--conf &lt;key&gt;=&lt;value&gt; --conf &lt;key2&gt;=&lt;value2&gt;</code>)</li><li>--driver-class-path: 驱动程序的类路径。</li><li>--application-arguments: 传递给主类的命令行参数。</li><li>--driver-memory: 驱动程序的内存。</li><li>--executor-memory: 每个 executor 的内存。</li><li>--executor-cores: 每个 executor 的 CPU 核心数。</li><li>--num-executors: 集群中 executor 的数量。</li><li>--archives: 上传到集群的归档文件。</li><li>--queue: 队列名。</li><li>--name: 作业名。</li><li>--verbose: 显示详细的日志信息。</li><li>--help: 显示帮助信息。</li></ul><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line"># Run application locally on 8 cores</span>
<span class="line">./bin/spark-submit \\</span>
<span class="line">  --class org.apache.spark.examples.SparkPi \\</span>
<span class="line">  --master local[8] \\</span>
<span class="line">  /path/to/examples.jar \\</span>
<span class="line">  100</span>
<span class="line"></span>
<span class="line"># Run on a Spark standalone cluster in client deploy mode</span>
<span class="line">./bin/spark-submit \\</span>
<span class="line">  --class org.apache.spark.examples.SparkPi \\</span>
<span class="line">  --master spark://0.0.0.0:7077 \\</span>
<span class="line">  --executor-memory 4G \\</span>
<span class="line">  --total-executor-cores 100 \\</span>
<span class="line">  /path/to/examples.jar \\</span>
<span class="line">  1000</span>
<span class="line"></span>
<span class="line"># Run on a Spark standalone cluster in cluster deploy mode with supervise</span>
<span class="line">./bin/spark-submit \\</span>
<span class="line">  --class org.apache.spark.examples.SparkPi \\</span>
<span class="line">  --master spark://0.0.0.0:7077 \\</span>
<span class="line">  --deploy-mode cluster \\</span>
<span class="line">  --supervise \\</span>
<span class="line">  --executor-memory 4G \\</span>
<span class="line">  --total-executor-cores 100 \\</span>
<span class="line">  /path/to/examples.jar \\</span>
<span class="line">  1000</span>
<span class="line"></span>
<span class="line"># Run a Python application on a Spark standalone cluster</span>
<span class="line">./bin/spark-submit \\</span>
<span class="line">  --master spark://0.0.0.0:7077 \\</span>
<span class="line">  examples/src/main/python/pi.py \\</span>
<span class="line">  1000</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="rdd-操作" tabindex="-1"><a class="header-anchor" href="#rdd-操作"><span>RDD 操作</span></a></h2><p>source 来源</p><h3 id="来源" tabindex="-1"><a class="header-anchor" href="#来源"><span>来源</span></a></h3><ol><li>本地集合</li></ol><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">val</span> data<span class="token operator">=</span>Array<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">,</span><span class="token number">5</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> rdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>data<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><ol start="2"><li>外部数据源</li></ol><p>此方法接受文件的 URI（机器上的本地路径或 hdfs://、s3a:// 等 URI）并将其读取为行集合</p><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">val</span> rdd<span class="token operator">=</span>sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;file:///path/to/file&quot;</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token comment">// Spark 的所有基于文件的输入方法（包括 textFile）都支持在目录、压缩文件和通配符上运行</span></span>
<span class="line">textFile<span class="token punctuation">(</span><span class="token string">&quot;/my/directory&quot;</span><span class="token punctuation">)</span></span>
<span class="line">textFile<span class="token punctuation">(</span><span class="token string">&quot;/my/directory/*.txt&quot;</span><span class="token punctuation">)</span> </span>
<span class="line">textFile<span class="token punctuation">(</span><span class="token string">&quot;/my/directory/*.gz&quot;</span><span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="转换" tabindex="-1"><a class="header-anchor" href="#转换"><span>转换</span></a></h2><p>RDD 支持两种类型的操作：转换，它从现有数据集创建新数据集，以及操作，它在对数据集运行计算后将值返回给驱动程序。</p><p>例如，</p><ul><li><p>map 是一种转换，它将每个数据集元素通过一个函数传递，并返回一个表示结果的新 RDD。</p></li><li><p>另一方面，reduce 是一种操作，它使用某个函数聚合 RDD 中的所有元素，并将最终结果返回给驱动程序（尽管也存在一个并行的 reduceByKey，它返回一个分布式数据集）。</p></li><li><p>惰性: Spark 中的所有转换都是惰性的，这意味着它们不会立即计算结果。相反，它们只记住应用于某个基本数据集（例如文件）的转换。只有当操作需要将结果返回给驱动程序时，才会计算转换。这种设计使 Spark 能够更有效地运行。例如，我们可以意识到，通过 map 创建的数据集将在 reduce 中使用，并且只将 reduce 的结果返回给驱动程序，而不是更大的映射数据集。</p></li><li><p>持久化： 默认情况下，每次对转换后的 RDD 运行操作时，它都可能被重新计算。但是，您也可以使用 persist（或 cache）方法将 RDD 持久化到内存中，在这种情况下，Spark 将在集群中保留这些元素，以便下次查询时能够更快地访问它们。还支持将 RDD 持久化到磁盘，或复制到多个节点上。</p></li></ul><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">val</span> lines <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;data/input.txt&quot;</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> lineLengths <span class="token operator">=</span> lines<span class="token punctuation">.</span>map<span class="token punctuation">(</span>s <span class="token keyword">=&gt;</span> s<span class="token punctuation">.</span>length<span class="token punctuation">)</span></span>
<span class="line"><span class="token comment">// 如果我们还想在以后再次使用 lineLengths，</span></span>
<span class="line"><span class="token comment">// 在 reduce 之前，这将导致 lineLengths 在第一次计算后保存在内存中</span></span>
<span class="line">lineLengths<span class="token punctuation">.</span>persist<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> totalLength <span class="token operator">=</span> lineLengths<span class="token punctuation">.</span>reduce<span class="token punctuation">(</span>_<span class="token operator">+</span>_<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="理解闭包" tabindex="-1"><a class="header-anchor" href="#理解闭包"><span>理解闭包</span></a></h2><ol><li>闭包</li></ol><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">def</span> add<span class="token punctuation">(</span>x<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Int</span> <span class="token operator">=</span> <span class="token punctuation">{</span></span>
<span class="line">  <span class="token keyword">def</span> addY<span class="token punctuation">(</span>y<span class="token operator">:</span> <span class="token builtin">Int</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Int</span> <span class="token operator">=</span> x <span class="token operator">+</span> y</span>
<span class="line">  addY</span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><ul><li>闭包是一个函数，它引用了其定义域之外的变量。</li><li>闭包可以访问其定义域中的变量，即使这些变量在函数返回后也仍然存在。</li><li>闭包可以作为参数传递给另一个函数，并在那里被调用。</li><li>闭包可以作为变量的值存储在数据结构中，并在稍后被访问。</li></ul><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">var</span> counter <span class="token operator">=</span> <span class="token number">0</span></span>
<span class="line"><span class="token keyword">var</span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>data<span class="token punctuation">)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">// Wrong: Don&#39;t do this!!</span></span>
<span class="line">rdd<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>x <span class="token keyword">=&gt;</span> counter <span class="token operator">+=</span> x<span class="token punctuation">)</span></span>
<span class="line"><span class="token comment">// 发送到每个执行器的闭包中的变量现在是副本，而不是共享变量</span></span>
<span class="line"><span class="token comment">// 解决使用 Accumulator 累加器</span></span>
<span class="line">println<span class="token punctuation">(</span><span class="token string">&quot;Counter value: &quot;</span> <span class="token operator">+</span> counter<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="打印-rdd-的元素" tabindex="-1"><a class="header-anchor" href="#打印-rdd-的元素"><span>打印 RDD 的元素</span></a></h3><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">rdd.foreach(println) 或 rdd.map(println) </span>
<span class="line"></span>
<span class="line">在一台机器上，这将生成预期的输出并打印所有 RDD 的元素。</span>
<span class="line">在 cluster 模式下，执行器调用的 stdout 现在写入执行器的 stdout 而不是驱动程序上的 stdout，因此驱动程序上的 stdout 不会显示这些</span>
<span class="line">要打印驱动程序上的所有元素，可以使用 collect() 方法首先将 RDD 带到驱动程序节点，如下所示：rdd.collect().foreach(println)。但这会导致驱动程序内存不足，因为 collect() 将整个 RDD 提取到一台机器上；如果您只需要打印 RDD 的几个元素，更安全的方法是使用</span>
<span class="line">take()：rdd.take(100).foreach(println)</span>
<span class="line"></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h3 id="使用键值对" tabindex="-1"><a class="header-anchor" href="#使用键值对"><span>使用键值对</span></a></h3><ol><li>键值对</li></ol><p>Spark 中的键值对通常表示为 (K,V) 对，其中 K 是键，V 是值。</p><ul><li>键可以是任意类型，但通常是一个不可变的类型（例如 Int、String 或 Tuple）。</li><li>值可以是任意类型。</li><li>键值对的集合可以表示为 RDD，其中每个元素是一个 (K,V) 对。</li><li>键值对的集合可以表示为 Map，其中每个键映射到一个值。</li></ul><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">val</span> pairs <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span>Array<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token string">&quot;a&quot;</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;b&quot;</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token string">&quot;a&quot;</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> map <span class="token operator">=</span> pairs<span class="token punctuation">.</span>collectAsMap<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">println<span class="token punctuation">(</span>map<span class="token punctuation">)</span> <span class="token comment">// Map(a -&gt; 3, b -&gt; 2)</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">// 以下代码使用 reduceByKey 操作对键值对进行操作，以计算文本文件中每行出现的次数。</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">val</span> lines <span class="token operator">=</span> sc<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;data/input.txt&quot;</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> counts <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>line <span class="token keyword">=&gt;</span> line<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot; &quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>map<span class="token punctuation">(</span>word <span class="token keyword">=&gt;</span> <span class="token punctuation">(</span>word<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>reduceByKey<span class="token punctuation">(</span>_ <span class="token operator">+</span> _<span class="token punctuation">)</span></span>
<span class="line">counts<span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>println<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="转换-1" tabindex="-1"><a class="header-anchor" href="#转换-1"><span>转换</span></a></h2><p>一些算子</p><ul><li>map: 对每个元素应用一个函数。</li><li>flatMap: 对每个元素应用一个函数，然后将结果展平为一个序列。</li><li>filter: 保留满足给定条件的元素。</li><li>distinct: 保留唯一的元素。</li><li>sample: 随机抽样元素。</li><li>union: 合并两个 RDD。</li><li>intersection: 保留两个 RDD 的交集。</li><li>subtract: 保留第一个 RDD 中不在第二个 RDD 中的元素。</li><li>groupByKey: 将键值对集合转换为键值对的集合，其中每个键对应于一个值集合。</li><li>reduceByKey: 聚合键相同的元素。</li><li>...</li><li>reduce(func): 对元素应用一个二元函数，并返回一个单一的值。</li><li>count: 返回 RDD 中元素的数量。</li><li>first: 返回 RDD 中第一个元素。</li><li>take(n): 返回 RDD 中前 n 个元素。</li><li>collect: 返回 RDD 中所有元素的集合。</li><li>foreach(func): 对每个元素应用一个函数。</li><li>...</li></ul><h2 id="洗牌操作" tabindex="-1"><a class="header-anchor" href="#洗牌操作"><span>洗牌操作</span></a></h2><p>Spark 提供了两种类型的洗牌操作：</p><ul><li>全局洗牌：对整个 RDD 进行洗牌。</li><li>局部洗牌：对每个分区进行洗牌。</li></ul><p>一些算子底层使用shuffle操作，例如 groupByKey、reduceByKey、join、cogroup、repartition等。</p><p>定义：</p><p>重新分配数据， 根据rdd,重新生成一个新的rdd，每个分区中的数据随机分配到其他分区，使得每个分区中的数据分布随机。 reduceByKey 操作生成一个新的 RDD，其中单个键的所有值都组合成一个元组 - 键和对与该键关联的所有值执行 reduce 函数的结果。</p><h2 id="rdd-持久化" tabindex="-1"><a class="header-anchor" href="#rdd-持久化"><span>RDD 持久化</span></a></h2><p>Spark 支持两种类型的持久化：</p><ul><li>cache(), 内存持久化：将 RDD 持久化到内存中。</li><li>persist(), 磁盘持久化：将 RDD 持久化到磁盘上。</li></ul><p>cache() 方法, 使用默认存储级别的简写，即 StorageLevel.MEMORY_ONLY. persist() 方法, 可以指定存储级别，</p><h3 id="存储级别" tabindex="-1"><a class="header-anchor" href="#存储级别"><span>存储级别</span></a></h3><ul><li>MEMORY_ONLY: 只缓存到内存中，将 RDD 存储为 JVM 中的反序列化 Java 对象。如果 RDD 无法容纳在内存中，则某些分区将不会被缓存，并且将在每次需要时动态重新计算。这是默认级别。</li><li>MEMORY_AND_DISK: 先缓存到内存中，再写入磁盘。将 RDD 存储为 JVM 中的反序列化 Java 对象。如果 RDD 无法容纳在内存中，则将无法容纳在磁盘上的分区存储在磁盘上，并在需要时从磁盘读取它们。</li><li>MEMORY_ONLY_SER: 只缓存到内存中，使用 Java 序列化。将 RDD 存储为序列化的 Java 对象（每个分区一个字节数组）。这通常比反序列化对象更节省空间，尤其是在使用 快速序列化器 时，但读取时更占用 CPU。</li><li>MEMORY_AND_DISK_SER: 与 MEMORY_ONLY_SER 相似，但将无法容纳在内存中的分区溢出到磁盘，而不是在每次需要时动态重新计算它们。</li><li>DISK_ONLY: 只写入磁盘，不缓存到内存中。</li><li>OFF_HEAP: 缓存到堆外内存，而不是 JVM 堆中。</li><li>MEMORY_ONLY_2: 与 MEMORY_ONLY 相同，但使用堆外内存。</li></ul><h3 id="删除数据" tabindex="-1"><a class="header-anchor" href="#删除数据"><span>删除数据</span></a></h3><ul><li>自动, LRU算法清除</li><li>手动, RDD.unpersist()， 不会阻塞</li></ul><h2 id="广播变量" tabindex="-1"><a class="header-anchor" href="#广播变量"><span>广播变量</span></a></h2><p>broadcast 就是将数据从一个节点发送到其他各个节点上去。</p><p>广播变量是只读变量，它可以在多个执行器上共享。</p><ul><li>广播变量可以用来在多个节点上缓存一个值，以便在多个任务之间共享。</li><li>广播变量可以用来在每个节点上缓存一个大型数据集，以便在多个任务之间共享。</li></ul><ol><li>为什么只能 broadcast 只读的变量</li></ol><p>一致性, 如果变量可以被更新，那么一旦变量被某个节点更新，其他节点要不要一块更新？如果多个节点同时在更新，更新顺序是什么？怎么做同步？还会涉及 fault-tolerance 的问题。为了避免维护数据一致性问题，Spark 目前只支持 broadcast 只读变量。</p><ol start="2"><li>broadcast 到节点而不是 broadcast 到每个 task？</li></ol><p>因为每个 task 是一个线程，而且同在一个进程运行 tasks 都属于同一个 application。因此每个节点（executor）上放一份就可以被所有 task 共享。</p><ol start="3"><li>如何使用 broadcast</li></ol><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">val</span> data <span class="token operator">=</span> List<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">6</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> bdata <span class="token operator">=</span> sc<span class="token punctuation">.</span>broadcast<span class="token punctuation">(</span>data<span class="token punctuation">)</span></span>
<span class="line"> </span>
<span class="line"><span class="token keyword">val</span> rdd <span class="token operator">=</span> sc<span class="token punctuation">.</span>parallelize<span class="token punctuation">(</span><span class="token number">1</span> to <span class="token number">6</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> observedSizes <span class="token operator">=</span> rdd<span class="token punctuation">.</span>map<span class="token punctuation">(</span>_ <span class="token keyword">=&gt;</span> bdata<span class="token punctuation">.</span>value<span class="token punctuation">.</span>size<span class="token punctuation">)</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="accumulator" tabindex="-1"><a class="header-anchor" href="#accumulator"><span>Accumulator</span></a></h2><p>Accumulator 是一种只写变量，它可以用来聚合多个任务的结果。<br> Accumulator 可以用来实现计数器、求和、平均值、列表、字典等。<br> Accumulator 不能被修改，只能被增加。<br> Accumulator 不能被广播，只能在每个节点上单独创建。<br> Accumulator 不能被 checkpoint，只能在 driver 节点上使用。<br> Accumulator 不能被持久化，只能在 driver 节点上使用。<br> Accumulator 不能被 cache，只能在 driver 节点上使用。<br> Accumulator 不能被使用 collect() 操作，只能在 driver 节点上使用.</p><p>累加器（Accumulators）与广播变量（Broadcast Variables）共同作为Spark提供的两大共享变量，主要用于跨集群的数据节点之间的数据共享，突破数据在集群各个executor不能共享问题。</p><p>而累加器主要定义在driver节点，在executor节点进行操作，最后在driver节点聚合结果做进一步的处理</p><p>常见的累加器:</p><ul><li>LongAccumulator: 参数支持Integer、Long</li><li>DoubleAccumulator: 参数支持Double</li><li>CollectionAccumulator: 参数支持List、Set、Map</li><li>AccumulatorV2: 自定义Accumulator，可以支持任意类型</li></ul><h3 id="累加器的使用场景" tabindex="-1"><a class="header-anchor" href="#累加器的使用场景"><span>累加器的使用场景:</span></a></h3><p>常用来某些统计类场景，比如统计最近1小时多少用户或 IP访问数量；监控某些灰黑产利用平台可能存在漏洞大肆薅羊毛，并短信或邮件通知相关人员及时采取对策，等相关场景会用到</p><p>driver 定义累加器，在各个executor上执行任务，累加器的值在各个executor上累加，最后在driver上聚合结果。</p><h3 id="累加器的实现" tabindex="-1"><a class="header-anchor" href="#累加器的实现"><span>累加器的实现</span></a></h3><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token comment">// 定义一个累加器, 对1、2、3、4求和</span></span>
<span class="line"><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span></span>SparkContext</span>
<span class="line"><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>rdd<span class="token punctuation">.</span></span>RDD</span>
<span class="line"><span class="token keyword">import</span> <span class="token namespace">org<span class="token punctuation">.</span>apache<span class="token punctuation">.</span>spark<span class="token punctuation">.</span>sql<span class="token punctuation">.</span></span><span class="token punctuation">{</span>DataFrame<span class="token punctuation">,</span> Row<span class="token punctuation">,</span> SparkSession<span class="token punctuation">}</span></span>
<span class="line"><span class="token keyword">import</span> <span class="token namespace">scala<span class="token punctuation">.</span>collection<span class="token punctuation">.</span></span>JavaConversions<span class="token punctuation">.</span>_</span>
<span class="line"></span>
<span class="line"><span class="token keyword">object</span> TestAccumulator <span class="token punctuation">{</span></span>
<span class="line">     <span class="token keyword">def</span> main<span class="token punctuation">(</span>args<span class="token operator">:</span> Array<span class="token punctuation">[</span><span class="token builtin">String</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span></span>
<span class="line">      <span class="token keyword">val</span> ss<span class="token operator">:</span> SparkSession <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>appName<span class="token punctuation">(</span><span class="token string">&quot;test-ccumulator&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">&quot;local[2]&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span></span>
<span class="line">      <span class="token keyword">val</span> sc <span class="token operator">=</span> ss<span class="token punctuation">.</span>sparkContext</span>
<span class="line">      <span class="token keyword">val</span> longAccumulator  <span class="token operator">=</span> sc<span class="token punctuation">.</span>longAccumulator<span class="token punctuation">(</span><span class="token string">&quot;My longAccumulator&quot;</span><span class="token punctuation">)</span></span>
<span class="line">      sc<span class="token punctuation">.</span>makeRDD<span class="token punctuation">(</span>Arrays<span class="token punctuation">.</span>asList<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span><span class="token number">2</span><span class="token punctuation">,</span><span class="token number">3</span><span class="token punctuation">,</span><span class="token number">4</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>foreach<span class="token punctuation">(</span>v <span class="token keyword">=&gt;</span> <span class="token punctuation">{</span></span>
<span class="line">        longAccumulator<span class="token punctuation">.</span>add<span class="token punctuation">(</span>v<span class="token punctuation">)</span></span>
<span class="line">        println<span class="token punctuation">(</span>Thread<span class="token punctuation">.</span>currentThread<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getName<span class="token operator">+</span><span class="token string">&quot; longAccumulator=&quot;</span><span class="token operator">+</span>longAccumulator<span class="token punctuation">.</span>value<span class="token punctuation">)</span> </span>
<span class="line">       <span class="token punctuation">}</span><span class="token punctuation">)</span></span>
<span class="line">      println<span class="token punctuation">(</span><span class="token string">&quot;Driver &quot;</span><span class="token operator">+</span>Thread<span class="token punctuation">.</span>currentThread<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getName<span class="token operator">+</span><span class="token string">&quot; longAccumulator=&quot;</span><span class="token operator">+</span>longAccumulator<span class="token punctuation">.</span>value<span class="token punctuation">)</span> </span>
<span class="line">  <span class="token punctuation">}</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><h2 id="启动-spark-作业" tabindex="-1"><a class="header-anchor" href="#启动-spark-作业"><span>启动 Spark 作业</span></a></h2><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh"><pre><code><span class="line"><span class="token comment">#类名传递给 Spark 的 bin/run-example 脚本运行 Java 和 Scala 示例</span></span>
<span class="line">./bin/run-example SparkPi</span>
<span class="line"></span>
<span class="line"><span class="token comment">#对于 Python 示例，请使用 spark-submit 代替</span></span>
<span class="line">./bin/spark-submit examples/src/main/python/pi.py <span class="token number">1000</span></span>
<span class="line"></span>
<span class="line"><span class="token comment">#对于 R 示例，请使用 spark-submit 代替</span></span>
<span class="line">./bin/spark-submit examples/src/main/r/dataframe.R</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div>`,91)]))}const o=n(l,[["render",t]]),u=JSON.parse('{"path":"/guide/etl/spark/01.rdd2.html","title":"RDD 编程指南","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"概述","slug":"概述","link":"#概述","children":[]},{"level":2,"title":"与 Spark 链接","slug":"与-spark-链接","link":"#与-spark-链接","children":[]},{"level":2,"title":"初始化 Spark","slug":"初始化-spark","link":"#初始化-spark","children":[]},{"level":2,"title":"使用 Shell","slug":"使用-shell","link":"#使用-shell","children":[]},{"level":2,"title":"RDD 操作","slug":"rdd-操作","link":"#rdd-操作","children":[{"level":3,"title":"来源","slug":"来源","link":"#来源","children":[]}]},{"level":2,"title":"转换","slug":"转换","link":"#转换","children":[]},{"level":2,"title":"理解闭包","slug":"理解闭包","link":"#理解闭包","children":[{"level":3,"title":"打印 RDD 的元素","slug":"打印-rdd-的元素","link":"#打印-rdd-的元素","children":[]},{"level":3,"title":"使用键值对","slug":"使用键值对","link":"#使用键值对","children":[]}]},{"level":2,"title":"转换","slug":"转换-1","link":"#转换-1","children":[]},{"level":2,"title":"洗牌操作","slug":"洗牌操作","link":"#洗牌操作","children":[]},{"level":2,"title":"RDD 持久化","slug":"rdd-持久化","link":"#rdd-持久化","children":[{"level":3,"title":"存储级别","slug":"存储级别","link":"#存储级别","children":[]},{"level":3,"title":"删除数据","slug":"删除数据","link":"#删除数据","children":[]}]},{"level":2,"title":"广播变量","slug":"广播变量","link":"#广播变量","children":[]},{"level":2,"title":"Accumulator","slug":"accumulator","link":"#accumulator","children":[{"level":3,"title":"累加器的使用场景:","slug":"累加器的使用场景","link":"#累加器的使用场景","children":[]},{"level":3,"title":"累加器的实现","slug":"累加器的实现","link":"#累加器的实现","children":[]}]},{"level":2,"title":"启动 Spark 作业","slug":"启动-spark-作业","link":"#启动-spark-作业","children":[]}],"git":{"updatedTime":1744888563000,"contributors":[{"name":"alice","username":"alice","email":"921757697@qq.com","commits":1,"url":"https://github.com/alice"}],"changelog":[{"hash":"d0aa97b762c5a828ab8e3a7802989f2039337caf","time":1744888563000,"email":"921757697@qq.com","author":"alice","message":"deploy"}]},"filePathRelative":"guide/etl/spark/01.rdd2.md"}');export{o as comp,u as data};
