import{_ as s}from"./20250420008-CWfxgdf_.js";import{_ as r,c as o,e as a,a as l,d as t,b as i,w as c,r as p,o as u}from"./app-DN6Q4pKI.js";const d={};function m(S,e){const n=p("RouteLink");return u(),o("div",null,[e[3]||(e[3]=a(`<h1 id="structured-streaming-之-source-解析" tabindex="-1"><a class="header-anchor" href="#structured-streaming-之-source-解析"><span>Structured Streaming 之 Source 解析</span></a></h1><p><strong><em>[ Spark] Structured Streaming 源码解析系列</em></strong></p><div class="language-markdown line-numbers-mode" data-highlighter="prismjs" data-ext="md"><pre><code><span class="line">本文内容适用范围：</span>
<span class="line"><span class="token list punctuation">*</span> 2018.11.02 update, Spark 2.4 全系列 √ (已发布：2.4.0)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div>`,3)),l("p",null,[e[1]||(e[1]=t("阅读本文前，请一定先阅读  ")),i(n,{to:"/guide/etl/spark/Structured-Streaming-Implementation-and-Overview.html"},{default:c(()=>e[0]||(e[0]=[t("Structured Streaming 实现思路与实现概述")])),_:1}),e[2]||(e[2]=t("  一文，其中概述了 Structured Streaming 的实现思路（包括 StreamExecution, Source, Sink 等在 Structured Streaming 里的作用），有了全局概念后再看本文的细节解释。"))]),e[4]||(e[4]=a(`<h2 id="引言" tabindex="-1"><a class="header-anchor" href="#引言"><span>引言</span></a></h2><p>Structured Streaming 非常显式地提出了输入(Source)、执行(StreamExecution)、输出(Sink)的 3 个组件，并且在每个组件显式地做到 fault-tolerant，由此得到整个 streaming 程序的 end-to-end exactly-once guarantees.</p><p>具体到源码上，Source 是一个抽象的接口 <a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala" target="_blank" rel="noopener noreferrer">trait Source</a> [1]，包括了 Structured Streaming 实现 end-to-end exactly-once 处理所一定需要提供的功能（我们将马上详细解析这些方法）：</p><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">trait</span> Source  <span class="token punctuation">{</span></span>
<span class="line">  <span class="token comment">/* 方法 (1) */</span> <span class="token keyword">def</span> schema<span class="token operator">:</span> StructType</span>
<span class="line">  <span class="token comment">/* 方法 (2) */</span> <span class="token keyword">def</span> getOffset<span class="token operator">:</span> Option<span class="token punctuation">[</span>Offset<span class="token punctuation">]</span></span>
<span class="line">  <span class="token comment">/* 方法 (3) */</span> <span class="token keyword">def</span> getBatch<span class="token punctuation">(</span>start<span class="token operator">:</span> Option<span class="token punctuation">[</span>Offset<span class="token punctuation">]</span><span class="token punctuation">,</span> end<span class="token operator">:</span> Offset<span class="token punctuation">)</span><span class="token operator">:</span> DataFrame</span>
<span class="line">  <span class="token comment">/* 方法 (4) */</span> <span class="token keyword">def</span> commit<span class="token punctuation">(</span>end<span class="token operator">:</span> Offset<span class="token punctuation">)</span> <span class="token operator">:</span> <span class="token builtin">Unit</span> <span class="token operator">=</span> <span class="token punctuation">{</span><span class="token punctuation">}</span></span>
<span class="line">  <span class="token comment">/* 方法 (5) */</span> <span class="token keyword">def</span> stop<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token operator">:</span> <span class="token builtin">Unit</span></span>
<span class="line"><span class="token punctuation">}</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>相比而言，前作 Spark Streaming 的输入 InputDStream 抽象 [2] 并不强制要求可靠和可重放，因而也存在一些不可靠输入源（如 Receiver-based 输入源），在失效情形下丢失源头输入数据；这时即使 Spark Streaming 框架本身能够重做，但由于源头数据已经不存在了，也会导致计算本身不是 exactly-once 的。当然，Spark Streaming 对可靠的数据源如 HDFS, Kafka 等的计算给出的 guarantee 还是 exactly-once。</p><p>进化到 Structured Streaming 后，只保留对 <strong><em>可靠数据源</em></strong> 的支持：</p><ul><li>已支持 <ul><li>Kafka，具体实现是 KafkaSource extends Source</li><li>HDFS-compatible file system，具体实现是 FileStreamSource extends Source</li><li>RateStream，具体实现是 RateStreamSource extends Source</li></ul></li><li>预计后续很快会支持 <ul><li>RDBMS</li></ul></li></ul><h2 id="source-方法与功能" tabindex="-1"><a class="header-anchor" href="#source-方法与功能"><span>Source：方法与功能</span></a></h2><p>在 Structured Streaming 里，由 StreamExecution 作为持续查询的驱动器，分批次不断地：</p><p><img src="`+s+'" alt="Spark 1.0"></p><ol><li>在每个 StreamExecution 的批次最开始，StreamExecution 会向 Source 询问当前 Source 的最新进度，即最新的 offset <ul><li>这里是由 StreamExecution 调用 Source 的 <code>def getOffset: Option[Offset]</code>，即方法 (2)</li><li>Kafka (KafkaSource) 的具体 <code>getOffset()</code> 实现 ，会通过在 driver 端的一个长时运行的 consumer 从 kafka brokers 处获取到各个 topic 最新的 offsets（注意这里不存在 driver 或 consumer 直接连 zookeeper），比如 <code>topicA_partition1:300, topicB_partition1:50, topicB_partition2:60</code>，并把 offsets 返回</li><li>HDFS-compatible file system (FileStreamSource) 的具体 <code>getOffset()</code> 实现，是先扫描一下最新的一组文件，给一个递增的编号并持久化下来，比如 <code>2 -&gt; {c.txt, d.txt}</code>，然后把编号 <code>2</code> 作为最新的 offset 返回</li></ul></li><li>这个 Offset 给到 StreamExecution 后会被 StreamExecution 持久化到自己的 WAL 里</li><li>由 Source 根据 StreamExecution 所要求的 start offset、end offset，提供在 <code>(start, end]</code> 区间范围内的数据 <ul><li>这里是由 StreamExecution 调用 Source 的 <code>def getBatch(start: Option[Offset], end: Offset): DataFrame</code>，即方法 (3)</li><li>这里的 start offset 和 end offset，通常就是 Source 在上一个执行批次里提供的最新 offset，和 Source 在这个批次里提供的最新 offset；但需要注意区间范围是 <strong><em>左开右闭</em></strong> ！</li><li>数据的返回形式的是一个 DataFrame（这个 DataFrame 目前只包含数据的描述信息，并没有发生实际的取数据操作）</li></ul></li><li>StreamExecution 触发计算逻辑 logicalPlan 的优化与编译</li><li>把计算结果写出给 Sink <ul><li>注意这时才会由 Sink 触发发生实际的取数据操作，以及计算过程</li></ul></li><li>在数据完整写出到 Sink 后，StreamExecution 通知 Source 可以废弃数据；然后把成功的批次 id 写入到 batchCommitLog <ul><li>这里是由 StreamExecution 调用 Source 的 <code>def commit(end: Offset): Unit</code>，即方法 (4)</li><li><code>commit()</code> 方法主要是帮助 Source 完成 garbage-collection，如果外部数据源本身即具有 garbage-collection 功能，如 Kafka，那么在 Source 的具体 <code>commit()</code> 实现上即可为空、留给外部数据源去自己管理</li></ul></li></ol><p>到此，是解析了 Source 的方法 (2) (3) (4) 在 StreamExecution 的具体批次执行中，所需要实现的语义和被调用的过程。</p><p>另外还有方法 (1) 和 (5)：</p><ul><li>方法 (1) <code>def schema: StructType</code><ul><li>返回一个本 Source 数据的 schema 描述，即每列数据的名称、类型、是否可空等</li><li>本方法在 Structured Streaming 开始真正执行每个批次开始前调用，不在每个批次执行时调用</li></ul></li><li>方法 (5) <code>def stop(): Unit</code><ul><li>当一个持续查询结束时，Source 会被调用此方法</li></ul></li></ul><h2 id="source-的具体实现-hdfs-compatible-file-system-kafka-rate" tabindex="-1"><a class="header-anchor" href="#source-的具体实现-hdfs-compatible-file-system-kafka-rate"><span>Source 的具体实现：HDFS-compatible file system, Kafka, Rate</span></a></h2><p>我们总结一下截至目前，Source 已有的具体实现：</p><table><thead><tr><th style="text-align:center;">Sources</th><th style="text-align:center;">是否可重放</th><th style="text-align:center;">原生内置支持</th><th style="text-align:center;">注解</th></tr></thead><tbody><tr><td style="text-align:center;"><strong>HDFS-compatible file system</strong></td><td style="text-align:center;">√</td><td style="text-align:center;">已支持</td><td style="text-align:center;">包括但不限于 text, json, csv, parquet, orc, ...</td></tr><tr><td style="text-align:center;"><strong>Kafka</strong></td><td style="text-align:center;">√</td><td style="text-align:center;">已支持</td><td style="text-align:center;">Kafka 0.10.0+</td></tr><tr><td style="text-align:center;"><strong>RateStream</strong></td><td style="text-align:center;">√</td><td style="text-align:center;">已支持</td><td style="text-align:center;">以一定速率产生数据</td></tr><tr><td style="text-align:center;"><strong>Socket</strong></td><td style="text-align:center;">×</td><td style="text-align:center;">已支持</td><td style="text-align:center;">主要用途是在技术会议/讲座上做 demo</td></tr></tbody></table><p>这里我们特别强调一下，虽然 Structured Streaming 也内置了 <code>socket</code> 这个 Source，但它并不能可靠重放、因而也不符合 Structured Streaming 的结构体系。它的主要用途只是在技术会议/讲座上做 demo，不应用于线上生产系统。</p><h2 id="扩展阅读" tabindex="-1"><a class="header-anchor" href="#扩展阅读"><span>扩展阅读</span></a></h2><ol><li><a href="https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html" target="_blank" rel="noopener noreferrer">Structured Streaming + Kafka Integration Guide (Kafka broker version 0.10.0 or higher)</a></li></ol><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2><ol><li><a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala" target="_blank" rel="noopener noreferrer">Github: org/apache/spark/sql/execution/streaming/Source.scala</a></li><li><a href="https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/dstream/InputDStream.scala" target="_blank" rel="noopener noreferrer">Github: org/apache/spark/streaming/dstream/InputDStream.scala</a></li></ol><br><br>',24))])}const k=r(d,[["render",m]]),h=JSON.parse('{"path":"/guide/etl/spark/Structured-Streaming-Source.html","title":"Structured Streaming 之 Source 解析","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"引言","slug":"引言","link":"#引言","children":[]},{"level":2,"title":"Source：方法与功能","slug":"source-方法与功能","link":"#source-方法与功能","children":[]},{"level":2,"title":"Source 的具体实现：HDFS-compatible file system, Kafka, Rate","slug":"source-的具体实现-hdfs-compatible-file-system-kafka-rate","link":"#source-的具体实现-hdfs-compatible-file-system-kafka-rate","children":[]},{"level":2,"title":"扩展阅读","slug":"扩展阅读","link":"#扩展阅读","children":[]},{"level":2,"title":"参考资料","slug":"参考资料","link":"#参考资料","children":[]}],"git":{"updatedTime":1753257234000,"contributors":[{"name":"alice","username":"alice","email":"921757697@qq.com","commits":3,"url":"https://github.com/alice"}],"changelog":[{"hash":"48409442c8c1f37ba42fad99036599dd52c923e4","time":1753257234000,"email":"921757697@qq.com","author":"alice","message":"deploy uniapp 组件"},{"hash":"245816fee7920b84913505a4353b6b4f934da7c0","time":1753237474000,"email":"921757697@qq.com","author":"alice","message":"uniapp 组件"},{"hash":"7706e85b299e47a4a064f79e4ce4f11f0e5f015c","time":1745114781000,"email":"921757697@qq.com","author":"alice","message":"doc 整理"}]},"filePathRelative":"guide/etl/spark/Structured-Streaming-Source.md"}');export{k as comp,h as data};
