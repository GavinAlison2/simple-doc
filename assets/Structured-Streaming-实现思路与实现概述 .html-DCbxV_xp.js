import{_ as e}from"./20250420008-CWfxgdf_.js";import{_ as a}from"./20250420009-DhL5IorG.js";import{_ as n}from"./20250420010-DhL5IorG.js";import{_ as s,c as r,e as l,o as i}from"./app-DlGl6QFf.js";const c="/simple-doc/assets/20250420001-Drhe0oQn.png",o="/simple-doc/assets/20250420002-GyAQjEUC.png",p="/simple-doc/assets/20250420003-3_eKM9LK.png",d="/simple-doc/assets/20250420004-D788kf7h.png",u="/simple-doc/assets/20250420005-qRdzVMYC.png",m="/simple-doc/assets/20250420006-D6oA7eiT.png",g="/simple-doc/assets/20250420007-DscRmcB1.png",h={};function k(S,t){return i(),r("div",null,t[0]||(t[0]=[l(`<h1 id="structured-streaming-实现思路与实现概述" tabindex="-1"><a class="header-anchor" href="#structured-streaming-实现思路与实现概述"><span>Structured Streaming 实现思路与实现概述</span></a></h1><p><em><strong>[Spark] Structured Streaming 源码解析系列</strong></em> ，返回目录请 <a href=".">猛戳这里</a></p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">本文内容适用范围：</span>
<span class="line">* 2018.11.02 update, Spark 2.4 全系列 √ (已发布：2.4.0)</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div></div></div><p>本文目录</p><div><a href="#一引言spark-2-时代">一、引言：Spark 2 时代!</a><br><a href="#二从-structured-data-到-structured-streaming">二、从 Structured Data 到 Structured Streaming</a><br><a href="#三structured-streaming无限增长的表格">三、Structured Streaming：无限增长的表格</a><br><a href="#四streamexecution持续查询的运转引擎">四、StreamExecution：持续查询的运转引擎</a><br> 　　<a href="#1-streamexecution-的初始状态">1. StreamExecution 的初始状态</a><br> 　　<a href="#2-streamexecution-的持续查询">2. StreamExecution 的持续查询</a><br> 　　<a href="#3-streamexecution-的持续查询增量">3. StreamExecution 的持续查询（增量）</a><br> 　　<a href="#4-故障恢复">4. 故障恢复</a><br> 　　<a href="#5-sources-与-sinks">5. Sources 与 Sinks</a><br> 　　<a href="#6-小结end-to-end-exactly-once-guarantees">6. 小结：end-to-end exactly-once guarantees</a><br><a href="#五全文总结">五、全文总结</a><br><a href="#六扩展阅读">六、扩展阅读</a><br><a href="#参考资料">参考资料</a></div><h2 id="一、引言-spark-2-时代" tabindex="-1"><a class="header-anchor" href="#一、引言-spark-2-时代"><span>一、引言：Spark 2 时代!</span></a></h2><p align="center"><img src="`+c+'" alt="Spark 1.x stack"></p><p>Spark 1.x 时代里，以 SparkContext（及 RDD API）为基础，在 structured data 场景衍生出了 SQLContext, HiveContext，在 streaming 场景衍生出了 StreamingContext，很是琳琅满目。</p><p align="center"><img src="'+o+'" alt="Spark 2.x stack"></p><p>Spark 2.x 则咔咔咔精简到只保留一个 SparkSession 作为主程序入口，以 Dataset/DataFrame 为主要的用户 API，同时满足 structured data, streaming data, machine learning, graph 等应用场景，大大减少使用者需要学习的内容，爽爽地又重新实现了一把当年的 &quot;one stack to rule them all&quot; 的理想。</p><p align="center"><img src="'+p+'" alt="RDD vs Dataset/DataFrame"></p><p>我们这里简单回顾下 Spark 2.x 的 Dataset/DataFrame 与 Spark 1.x 的 RDD 的不同：</p><ul><li>Spark 1.x 的 RDD 更多意义上是一个一维、只有行概念的数据集，比如 <code>RDD[Person]</code>，那么一行就是一个 <code>Person</code>，存在内存里也是把 <code>Person</code> 作为一个整体（序列化前的 java object，或序列化后的 bytes）。</li><li>Spark 2.x 里，一个 <code>Person</code> 的 Dataset 或 DataFrame，是二维行+列的数据集，比如一行一个 <code>Person</code>，有 <code>name:String</code>, <code>age:Int</code>, <code>height:Double</code> 三列；在内存里的物理结构，也会显式区分列边界。 <ul><li>Dataset/DataFrame 在 API 使用上有区别：Dataset 相比 DataFrame 而言是 type-safe 的，能够在编译时对 AnalysisExecption 报错（如下图示例）: <img src="'+d+'"></li><li>Dataset/DataFrame 存储方式无区别：两者在内存中的存储方式是完全一样的、是按照二维行列（UnsafeRow）来存的，所以在没必要区分 <code>Dataset</code> 或 <code>DataFrame</code> 在 API 层面的差别时，我们统一写作 <code>Dataset/DataFrame</code></li></ul></li></ul><blockquote><p>[小节注] 其实 Spark 1.x 就有了 Dataset/DataFrame 的概念，但还仅是 SparkSQL 模块的主要 API ；到了 2.0 时则 Dataset/DataFrame 不局限在 SparkSQL、而成为 Spark 全局的主要 API。</p></blockquote><h2 id="二、从-structured-data-到-structured-streaming" tabindex="-1"><a class="header-anchor" href="#二、从-structured-data-到-structured-streaming"><span>二、从 Structured Data 到 Structured Streaming</span></a></h2><p>使用 Dataset/DataFrame 的行列数据表格来表达 structured data，既容易理解，又具有广泛的适用性：</p><ul><li>Java 类 <code>class Person { String name; int age; double height}</code> 的多个对象可以方便地转化为 <code>Dataset/DataFrame</code></li><li>多条 json 对象比如 <code>{name: &quot;Alice&quot;, age: 20, height: 1.68}, {name: &quot;Bob&quot;, age: 25, height: 1.76}</code> 可以方便地转化为 <code>Dataset/DataFrame</code></li><li>或者 MySQL 表、行式存储文件、列式存储文件等等等都可以方便地转化为 <code>Dataset/DataFrame</code></li></ul><p>Spark 2.0 更进一步，使用 Dataset/Dataframe 的行列数据表格来扩展表达 streaming data —— 所以便横空出世了 Structured Streaming 、《Structured Streaming 源码解析系列》—— 与静态的 structured data 不同，动态的 streaming data 的行列数据表格是一直无限增长的（因为 streaming data 在源源不断地产生）！</p><p align="center"><img src="'+u+'" alt="single API: Dataset/DataFrame"></p><h2 id="三、structured-streaming-无限增长的表格" tabindex="-1"><a class="header-anchor" href="#三、structured-streaming-无限增长的表格"><span>三、Structured Streaming：无限增长的表格</span></a></h2><p>基于“无限增长的表格”的编程模型 [1]，我们来写一个 streaming 的 word count：</p><p align="center"><img src="'+m+`" alt="word count"></p><p>对应的 Structured Streaming 代码片段：</p><div class="language-scala line-numbers-mode" data-highlighter="prismjs" data-ext="scala"><pre><code><span class="line"><span class="token keyword">val</span> spark <span class="token operator">=</span> SparkSession<span class="token punctuation">.</span>builder<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>master<span class="token punctuation">(</span><span class="token string">&quot;...&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>getOrCreate<span class="token punctuation">(</span><span class="token punctuation">)</span>  <span class="token comment">// 创建一个 SparkSession 程序入口</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">val</span> lines <span class="token operator">=</span> spark<span class="token punctuation">.</span>readStream<span class="token punctuation">.</span>textFile<span class="token punctuation">(</span><span class="token string">&quot;some_dir&quot;</span><span class="token punctuation">)</span>  <span class="token comment">// 将 some_dir 里的内容创建为 Dataset/DataFrame；即 input table</span></span>
<span class="line"><span class="token keyword">val</span> words <span class="token operator">=</span> lines<span class="token punctuation">.</span>flatMap<span class="token punctuation">(</span>_<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">&quot; &quot;</span><span class="token punctuation">)</span><span class="token punctuation">)</span></span>
<span class="line"><span class="token keyword">val</span> wordCounts <span class="token operator">=</span> words<span class="token punctuation">.</span>groupBy<span class="token punctuation">(</span><span class="token string">&quot;value&quot;</span><span class="token punctuation">)</span><span class="token punctuation">.</span>count<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token comment">// 对 &quot;value&quot; 列做 count，得到多行二列的 Dataset/DataFrame；即 result table</span></span>
<span class="line"></span>
<span class="line"><span class="token keyword">val</span> query <span class="token operator">=</span> wordCounts<span class="token punctuation">.</span>writeStream                 <span class="token comment">// 打算写出 wordCounts 这个 Dataset/DataFrame</span></span>
<span class="line">  <span class="token punctuation">.</span>outputMode<span class="token punctuation">(</span><span class="token string">&quot;complete&quot;</span><span class="token punctuation">)</span>                          <span class="token comment">// 打算写出 wordCounts 的全量数据</span></span>
<span class="line">  <span class="token punctuation">.</span>format<span class="token punctuation">(</span><span class="token string">&quot;console&quot;</span><span class="token punctuation">)</span>                               <span class="token comment">// 打算写出到控制台</span></span>
<span class="line">  <span class="token punctuation">.</span>start<span class="token punctuation">(</span><span class="token punctuation">)</span>                                         <span class="token comment">// 新起一个线程开始真正不停写出</span></span>
<span class="line"></span>
<span class="line">query<span class="token punctuation">.</span>awaitTermination<span class="token punctuation">(</span><span class="token punctuation">)</span>                           <span class="token comment">// 当前用户主线程挂住，等待新起来的写出线程结束</span></span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里需要说明几点：</p><ul><li>Structured Streaming 也是先纯定义、再触发执行的模式，即 <ul><li>前面大部分代码是 <em><strong>纯定义</strong></em> Dataset/DataFrame 的产生、变换和写出</li><li>后面位置再真正 <em><strong>start</strong></em> 一个新线程，去触发执行之前的定义</li></ul></li><li>在新的执行线程里我们需要 <em><strong>持续地</strong></em> 去发现新数据，进而 <em><strong>持续地</strong></em> 查询最新计算结果至写出 <ul><li>这个过程叫做 <em><strong>continous query（持续查询）</strong></em></li></ul></li></ul><h2 id="四、streamexecution-持续查询的运转引擎" tabindex="-1"><a class="header-anchor" href="#四、streamexecution-持续查询的运转引擎"><span>四、StreamExecution：持续查询的运转引擎</span></a></h2><p>现在我们将目光聚焦到 <em><strong>continuous query</strong></em> 的驱动引擎（即整个 Structured Streaming 的驱动引擎） StreamExecution 上来。</p><h3 id="_1-streamexecution-的初始状态" tabindex="-1"><a class="header-anchor" href="#_1-streamexecution-的初始状态"><span>1. StreamExecution 的初始状态</span></a></h3><p>我们前文刚解析过，先定义好 Dataset/DataFrame 的产生、变换和写出，再启动 StreamExection 去持续查询。这些 Dataset/DataFrame 的产生、变换和写出的信息就对应保存在 StreamExecution 非常重要的 3 个成员变量中：</p><ul><li><code>sources</code>: streaming data 的产生端（比如 kafka 等）</li><li><code>logicalPlan</code>: DataFrame/Dataset 的一系列变换（即计算逻辑）</li><li><code>sink</code>: 最终结果写出的接收端（比如 file system 等）</li></ul><p>StreamExection 另外的重要成员变量是：</p><ul><li><code>currentBatchId</code>: 当前执行的 id</li><li><code>batchCommitLog</code>: 已经成功处理过的批次有哪些</li><li><code>offsetLog</code>, <code>availableOffsets</code>, <code>committedOffsets</code>: 当前执行需要处理的 source data 的 meta 信息</li><li><code>offsetSeqMetadata</code>: 当前执行的 watermark 信息（event time 相关，本文暂不涉及、另文解析）等</li></ul><p>我们将 Source, Sink, StreamExecution 及其重要成员变量标识在下图，接下来将逐个详细解析。</p><p><img src="`+g+'" alt="Spark 1.0"></p><h3 id="_2-streamexecution-的持续查询" tabindex="-1"><a class="header-anchor" href="#_2-streamexecution-的持续查询"><span>2. StreamExecution 的持续查询</span></a></h3><p><img src="'+e+'" alt="Spark 1.0"></p><p>一次执行的过程如上图；这里有 6 个关键步骤：</p><ol><li>StreamExecution 通过 Source.getOffset() 获取最新的 offsets，即最新的数据进度；</li><li>StreamExecution 将 offsets 等写入到 offsetLog 里 <ul><li>这里的 offsetLog 是一个持久化的 WAL (Write-Ahead-Log)，是将来可用作故障恢复用</li></ul></li><li>StreamExecution 构造本次执行的 LogicalPlan <ul><li>(3a) 将预先定义好的逻辑（即 StreamExecution 里的 logicalPlan 成员变量）制作一个副本出来</li><li>(3b) 给定刚刚取到的 offsets，通过 Source.getBatch(offsets) 获取本执行新收到的数据的 Dataset/DataFrame 表示，并替换到 (3a) 中的副本里</li><li>经过 (3a), (3b) 两步，构造完成的 LogicalPlan 就是针对本执行新收到的数据的 Dataset/DataFrame 变换（即整个处理逻辑）了</li></ul></li><li>触发对本次执行的 LogicalPlan 的优化，得到 IncrementalExecution <ul><li>逻辑计划的优化：通过 Catalyst 优化器完成</li><li>物理计划的生成与选择：结果是可以直接用于执行的 RDD DAG</li><li>逻辑计划、优化的逻辑计划、物理计划、及最后结果 RDD DAG，合并起来就是 IncrementalExecution</li></ul></li><li>将表示计算结果的 Dataset/DataFrame (包含 IncrementalExecution) 交给 Sink，即调用 Sink.add(ds/df)</li><li>计算完成后的 commit <ul><li>(6a) 通过 Source.commit() 告知 Source 数据已经完整处理结束；Source 可按需完成数据的 garbage-collection</li><li>(6b) 将本次执行的批次 id 写入到 batchCommitLog 里</li></ul></li></ol><h3 id="_3-streamexecution-的持续查询-增量" tabindex="-1"><a class="header-anchor" href="#_3-streamexecution-的持续查询-增量"><span>3. StreamExecution 的持续查询（增量）</span></a></h3><p><img src="'+a+'" alt="Spark 1.0"></p><p>Structured Streaming 在编程模型上暴露给用户的是，每次持续查询看做面对全量数据（而不仅仅是本次执行信收到的数据），所以每次执行的结果是针对全量数据进行计算的结果。</p><p>但是在实际执行过程中，由于全量数据会越攒越多，那么每次对全量数据进行计算的代价和消耗会越来越大。</p><p>Structured Streaming 的做法是：</p><ul><li>引入全局范围、高可用的 StateStore</li><li>转全量为增量，即在每次执行时： <ul><li>先从 StateStore 里 restore 出上次执行后的状态</li><li>然后加入本执行的新数据，再进行计算</li><li>如果有状态改变，将把改变的状态重新 save 到 StateStore 里</li></ul></li><li>为了在 Dataset/DataFrame 框架里完成对 StateStore 的 restore 和 save 操作，引入两个新的物理计划节点 —— StateStoreRestoreExec 和 StateStoreSaveExec</li></ul><p>所以 Structured Streaming 在编程模型上暴露给用户的是，每次持续查询看做面对全量数据，但在具体实现上转换为增量的持续查询。</p><h3 id="_4-故障恢复" tabindex="-1"><a class="header-anchor" href="#_4-故障恢复"><span>4. 故障恢复</span></a></h3><p>通过前面小节的解析，我们知道存储 source offsets 的 offsetLog，和存储计算状态的 StateStore，是全局高可用的。仍然采用前面的示意图，offsetLog 和 StateStore 被特殊标识为紫色，代表高可用。</p><p><img src="'+n+`" alt="Spark 1.0"></p><p>由于 exectutor 节点的故障可由 Spark 框架本身很好的 handle，不引起可用性问题，我们本节的故障恢复只讨论 driver 故障恢复。</p><p>如果在某个执行过程中发生 driver 故障，那么重新起来的 StreamExecution：</p><ul><li>读取 WAL offsetlog 恢复出最新的 offsets 等；相当于取代正常流程里的 (1)(2) 步</li><li>读取 batchCommitLog 决定是否需要重做最近一个批次</li><li>如果需要，那么重做 (3a), (3b), (4), (5), (6a), (6b) 步 <ul><li>这里第 (5) 步需要分两种情况讨论 <ul><li>(i) 如果上次执行在 (5) <em><strong>结束前即失效</strong></em>，那么本次执行里 sink 应该完整写出计算结果</li><li>(ii) 如果上次执行在 (5) <em><strong>结束后才失效</strong></em>，那么本次执行里 sink 可以重新写出计算结果（覆盖上次结果），也可以跳过写出计算结果（因为上次执行已经完整写出过计算结果了）</li></ul></li></ul></li></ul><p>这样即可保证每次执行的计算结果，在 sink 这个层面，是 <em><strong>不重不丢</strong></em> 的 —— 即使中间发生过 1 次或以上的失效和恢复。</p><h3 id="_5-sources-与-sinks" tabindex="-1"><a class="header-anchor" href="#_5-sources-与-sinks"><span>5. Sources 与 Sinks</span></a></h3><p>可以看到，Structured Streaming 层面的 Source，需能 <em><strong>根据 offsets 重放数据</strong></em> [2]。所以：</p><table><thead><tr><th style="text-align:center;">Sources</th><th style="text-align:center;">是否可重放</th><th style="text-align:center;">原生内置支持</th><th style="text-align:center;">注解</th></tr></thead><tbody><tr><td style="text-align:center;"><strong>HDFS-compatible file system</strong></td><td style="text-align:center;">√</td><td style="text-align:center;">已支持</td><td style="text-align:center;">包括但不限于 text, json, csv, parquet, orc, ...</td></tr><tr><td style="text-align:center;"><strong>Kafka</strong></td><td style="text-align:center;">√</td><td style="text-align:center;">已支持</td><td style="text-align:center;">Kafka 0.10.0+</td></tr><tr><td style="text-align:center;"><strong>RateStream</strong></td><td style="text-align:center;">√</td><td style="text-align:center;">已支持</td><td style="text-align:center;">以一定速率产生数据</td></tr><tr><td style="text-align:center;"><strong>RDBMS</strong></td><td style="text-align:center;">√</td><td style="text-align:center;"><em>(待支持)</em></td><td style="text-align:center;">预计后续很快会支持</td></tr><tr><td style="text-align:center;"><strong>Socket</strong></td><td style="text-align:center;">×</td><td style="text-align:center;">已支持</td><td style="text-align:center;">主要用途是在技术会议/讲座上做 demo</td></tr><tr><td style="text-align:center;"><strong>Receiver-based</strong></td><td style="text-align:center;">×</td><td style="text-align:center;">不会支持</td><td style="text-align:center;">就让这些前浪被拍在沙滩上吧</td></tr></tbody></table><p>也可以看到，Structured Streaming 层面的 Sink，需能 <em><strong>幂等式写入数据</strong></em> [3]。所以：</p><table><thead><tr><th style="text-align:center;">Sinks</th><th style="text-align:center;">是否幂等写入</th><th style="text-align:center;">原生内置支持</th><th style="text-align:center;">注解</th></tr></thead><tbody><tr><td style="text-align:center;"><strong>HDFS-compatible file system</strong></td><td style="text-align:center;">√</td><td style="text-align:center;">已支持</td><td style="text-align:center;">包括但不限于 text, json, csv, parquet, orc, ...</td></tr><tr><td style="text-align:center;"><strong>ForeachSink</strong> (自定操作幂等)</td><td style="text-align:center;">√</td><td style="text-align:center;">已支持</td><td style="text-align:center;">可定制度非常高的 sink</td></tr><tr><td style="text-align:center;"><strong>RDBMS</strong></td><td style="text-align:center;">√</td><td style="text-align:center;"><em>(待支持)</em></td><td style="text-align:center;">预计后续很快会支持</td></tr><tr><td style="text-align:center;"><strong>Kafka</strong></td><td style="text-align:center;">×</td><td style="text-align:center;">已支持</td><td style="text-align:center;">Kafka 目前不支持幂等写入，所以可能会有重复写入<br>（但推荐接着 Kafka 使用 streaming de-duplication 来去重）</td></tr><tr><td style="text-align:center;"><strong>ForeachSink</strong> (自定操作不幂等)</td><td style="text-align:center;">×</td><td style="text-align:center;">已支持</td><td style="text-align:center;">不推荐使用不幂等的自定操作</td></tr><tr><td style="text-align:center;"><strong>Console</strong></td><td style="text-align:center;">×</td><td style="text-align:center;">已支持</td><td style="text-align:center;">主要用途是在技术会议/讲座上做 demo</td></tr></tbody></table><h3 id="_6-小结-end-to-end-exactly-once-guarantees" tabindex="-1"><a class="header-anchor" href="#_6-小结-end-to-end-exactly-once-guarantees"><span>6. 小结：end-to-end exactly-once guarantees</span></a></h3><p>所以在 Structured Streaming 里，我们总结下面的关系[4]：</p><div class="language-text line-numbers-mode" data-highlighter="prismjs" data-ext="text"><pre><code><span class="line">- offset tracking in wal</span>
<span class="line">- state management in state store</span>
<span class="line">- fault-tolerance sources and sinks</span>
<span class="line">= end to end exactly-once guarantees</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>这里的 end-to-end 指的是，如果 source 选用类似 Kafka, HDFS 等，sink 选用类似 HDFS, MySQL 等，那么 Structured Streaming 将自动保证在 sink 里的计算结果是 exactly-once 的 —— Structured Streaming 终于把过去需要使用者去维护的 sink 去重逻辑接盘过去了！😃</p><h2 id="五、全文总结" tabindex="-1"><a class="header-anchor" href="#五、全文总结"><span>五、全文总结</span></a></h2><p>自 Spark 2.0 开始，处理 structured data 的 Dateset/DataFrame 被扩展为同时处理 streaming data，诞生了 Structured Streaming。</p><p>Structured Streaming 以“无限扩展的表格”为编程模型，在 StreamExecution 实际执行中增量执行，并满足 end-to-end exactly-once guarantee.</p><p>在 Spark 2.0 时代，Dataset/DataFrame 成为主要的用户 API，同时满足 structured data, streaming data, machine learning, graph 等应用场景，大大减少使用者需要学习的内容，爽爽地又重新实现了一把当年的 &quot;one stack to rule them all&quot; 的理想。</p><blockquote><p>谨以此《Structured Streaming 源码解析系列》和以往的《Spark Streaming 源码解析系列》，向“把大数据变得更简单 (make big data simple) ”的创新者们，表达感谢和敬意。</p></blockquote><h2 id="六、扩展阅读" tabindex="-1"><a class="header-anchor" href="#六、扩展阅读"><span>六、扩展阅读</span></a></h2><ol><li>Spark Summit East 2016: <a href="https://spark-summit.org/east-2016/events/keynote-day-3/" target="_blank" rel="noopener noreferrer">The Future of Real-time in Spark</a></li><li>Blog: <a href="https://databricks.com/blog/2016/07/28/continuous-applications-evolving-streaming-in-apache-spark-2-0.html" target="_blank" rel="noopener noreferrer">Continuous Applications: Evolving Streaming in Apache Spark 2.0</a></li><li>Blog: <a href="https://databricks.com/blog/2016/07/28/structured-streaming-in-apache-spark.html" target="_blank" rel="noopener noreferrer">Structured Streaming In Apache Spark: A new high-level API for streaming</a></li></ol><h2 id="参考资料" tabindex="-1"><a class="header-anchor" href="#参考资料"><span>参考资料</span></a></h2><ol><li><a href="http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html" target="_blank" rel="noopener noreferrer">Structured Streaming Programming Guide</a></li><li><a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Source.scala" target="_blank" rel="noopener noreferrer">Github: org/apache/spark/sql/execution/streaming/Source.scala</a></li><li><a href="https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/streaming/Sink.scala" target="_blank" rel="noopener noreferrer">Github: org/apache/spark/sql/execution/streaming/Sink.scala</a></li><li><a href="http://www.slideshare.net/databricks/a-deep-dive-into-structured-streaming?qid=51953136-8233-4d5d-a1c2-ce30051f16d1&amp;v=&amp;b=&amp;from_search=1" target="_blank" rel="noopener noreferrer">A Deep Dive into Structured Streaming</a></li></ol><h2 id="知识共享" tabindex="-1"><a class="header-anchor" href="#知识共享"><span>知识共享</span></a></h2><p><img src="https://licensebuttons.net/l/by-nc/4.0/88x31.png" alt=""></p><p>除非另有注明，本《Structured Streaming 源码解析系列》系列文章使用 <a href="https://creativecommons.org/licenses/by-nc/4.0/" target="_blank" rel="noopener noreferrer">CC BY-NC（署名-非商业性使用）</a> 知识共享许可协议。<br><br></p><blockquote><p>https://github.com/lw-lin/CoolplaySpark</p></blockquote>`,75)]))}const v=s(h,[["render",k]]),D=JSON.parse('{"path":"/guide/etl/spark/Structured-Streaming-%E5%AE%9E%E7%8E%B0%E6%80%9D%E8%B7%AF%E4%B8%8E%E5%AE%9E%E7%8E%B0%E6%A6%82%E8%BF%B0%20.html","title":"Structured Streaming 实现思路与实现概述","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"一、引言：Spark 2 时代!","slug":"一、引言-spark-2-时代","link":"#一、引言-spark-2-时代","children":[]},{"level":2,"title":"二、从 Structured Data 到 Structured Streaming","slug":"二、从-structured-data-到-structured-streaming","link":"#二、从-structured-data-到-structured-streaming","children":[]},{"level":2,"title":"三、Structured Streaming：无限增长的表格","slug":"三、structured-streaming-无限增长的表格","link":"#三、structured-streaming-无限增长的表格","children":[]},{"level":2,"title":"四、StreamExecution：持续查询的运转引擎","slug":"四、streamexecution-持续查询的运转引擎","link":"#四、streamexecution-持续查询的运转引擎","children":[{"level":3,"title":"1. StreamExecution 的初始状态","slug":"_1-streamexecution-的初始状态","link":"#_1-streamexecution-的初始状态","children":[]},{"level":3,"title":"2. StreamExecution 的持续查询","slug":"_2-streamexecution-的持续查询","link":"#_2-streamexecution-的持续查询","children":[]},{"level":3,"title":"3. StreamExecution 的持续查询（增量）","slug":"_3-streamexecution-的持续查询-增量","link":"#_3-streamexecution-的持续查询-增量","children":[]},{"level":3,"title":"4. 故障恢复","slug":"_4-故障恢复","link":"#_4-故障恢复","children":[]},{"level":3,"title":"5. Sources 与 Sinks","slug":"_5-sources-与-sinks","link":"#_5-sources-与-sinks","children":[]},{"level":3,"title":"6. 小结：end-to-end exactly-once guarantees","slug":"_6-小结-end-to-end-exactly-once-guarantees","link":"#_6-小结-end-to-end-exactly-once-guarantees","children":[]}]},{"level":2,"title":"五、全文总结","slug":"五、全文总结","link":"#五、全文总结","children":[]},{"level":2,"title":"六、扩展阅读","slug":"六、扩展阅读","link":"#六、扩展阅读","children":[]},{"level":2,"title":"参考资料","slug":"参考资料","link":"#参考资料","children":[]},{"level":2,"title":"知识共享","slug":"知识共享","link":"#知识共享","children":[]}],"git":{"updatedTime":1753237474000,"contributors":[{"name":"alice","username":"alice","email":"921757697@qq.com","commits":2,"url":"https://github.com/alice"}],"changelog":[{"hash":"245816fee7920b84913505a4353b6b4f934da7c0","time":1753237474000,"email":"921757697@qq.com","author":"alice","message":"uniapp 组件"},{"hash":"7706e85b299e47a4a064f79e4ce4f11f0e5f015c","time":1745114781000,"email":"921757697@qq.com","author":"alice","message":"doc 整理"}]},"filePathRelative":"guide/etl/spark/Structured-Streaming-实现思路与实现概述 .md"}');export{v as comp,D as data};
