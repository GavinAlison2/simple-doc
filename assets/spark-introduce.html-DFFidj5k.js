import{_ as a,c as i,e,o as r}from"./app-CdVidjIO.js";const s={};function n(p,l){return r(),i("div",null,l[0]||(l[0]=[e('<h1 id="spark" tabindex="-1"><a class="header-anchor" href="#spark"><span>Spark</span></a></h1><ul><li><a href="#spark-%E7%AE%80%E4%BB%8B">Spark 简介</a><ul><li><a href="#spark-%E6%A6%82%E5%BF%B5">Spark 概念</a></li><li><a href="#spark-%E7%89%B9%E7%82%B9">Spark 特点</a></li></ul></li><li><a href="#spark-%E5%8E%9F%E7%90%86">Spark 原理</a><ul><li><a href="#%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B">编程模型</a></li></ul></li></ul><h2 id="spark-简介" tabindex="-1"><a class="header-anchor" href="#spark-简介"><span>Spark 简介</span></a></h2><h3 id="spark-概念" tabindex="-1"><a class="header-anchor" href="#spark-概念"><span>Spark 概念</span></a></h3><ul><li>大规模分布式通用计算引擎 <ul><li>Spark Core：核心计算框架</li><li>Spark SQL：结构化数据查询</li><li>Spark Streaming：实时流处理</li><li>Spark MLib：机器学习</li><li>Spark GraphX：图计算</li></ul></li><li>具有高吞吐、低延时、通用易扩展、高容错等特点</li><li>采用 Scala 语言开发</li><li>提供多种运行模式</li></ul><h3 id="spark-特点" tabindex="-1"><a class="header-anchor" href="#spark-特点"><span>Spark 特点</span></a></h3><ul><li>计算高效 <ul><li>利用内存计算、Cache 缓存机制，支持迭代计算和数据共享，减少数据读取的 IO 开销</li><li>利用 DAG 引擎，减少中间计算结果写入 HDFS 的开销</li><li>利用多线程池模型，减少任务启动开销，避免 Shuffle 中不必要的排序和磁盘 IO 操作</li></ul></li><li>通用易用 <ul><li>适用于批处理、流处理、交互式计算、机器学习算法等场景</li><li>提供了丰富的开发 API，支持 Scala、Java、Python、R 等</li></ul></li><li>运行模式多样 <ul><li>Local 模式</li><li>Standalone 模式</li><li>YARN/Mesos 模式</li></ul></li><li>计算高效 <ul><li>利用内存计算、Cache 缓存机制，支持迭代计算和数据共享，减少数据读取的 IO 开销</li><li>利用 DAG 引擎，减少中间计算结果写入 HDFS 的开销</li><li>利用多线程池模型，减少任务启动开销，避免 Shuffle 中不必要的排序和磁盘 IO 操作</li></ul></li><li>通用易用 <ul><li>适用于批处理、流处理、交互式计算、机器学习等场景</li><li>提供了丰富的开发 API，支持 Scala、Java、Python、R 等</li></ul></li></ul><h2 id="spark-原理" tabindex="-1"><a class="header-anchor" href="#spark-原理"><span>Spark 原理</span></a></h2><h3 id="编程模型" tabindex="-1"><a class="header-anchor" href="#编程模型"><span>编程模型</span></a></h3><h4 id="rdd" tabindex="-1"><a class="header-anchor" href="#rdd"><span>RDD</span></a></h4><ul><li>弹性分布式数据集（Resilient Distributed Datesets） <ul><li>分布在集群中的只读对象集合</li><li>由多个 Partition 组成</li><li>通过转换操作构造</li><li>失效后自动重构（弹性）</li><li>存储在内存或磁盘中</li></ul></li><li>Spark 基于 RDD 进行计算</li></ul><h4 id="rdd-操作-operator" tabindex="-1"><a class="header-anchor" href="#rdd-操作-operator"><span>RDD 操作（Operator）</span></a></h4><ul><li>Transformation（转换） <ul><li>将 Scala 集合或 Hadoop 输入数据构造成一个新 RDD</li><li>通过已有的 RDD 产生新 RDD</li><li>惰性执行：只记录转换关系，不触发计算</li><li>例如：map、filter、flatmap、union、distinct、sortbykey</li></ul></li><li>Action（动作） <ul><li>通过 RDD 计算得到一个值或一组值</li><li>真正触发计算</li><li>例如：first、count、collect、foreach、saveAsTextFile</li></ul></li></ul><h4 id="rdd-依赖-dependency" tabindex="-1"><a class="header-anchor" href="#rdd-依赖-dependency"><span>RDD 依赖（Dependency）</span></a></h4><ul><li>窄依赖（Narrow Dependency） <ul><li>父 RDD 中的分区最多只能被一个子 RDD 的一个分区使用</li><li>子 RDD 如果有部分分区数据丢失或损坏，只需从对应的父 RDD 重新计算恢复</li><li>例如：map、filter、union</li></ul></li><li>宽依赖（Shuffle/Wide Dependency ） <ul><li>子 RDD 分区依赖父 RDD 的所有分区</li><li>子 RDD 如果部分或全部分区数据丢失或损坏，必须从所有父 RDD 分区重新计算</li><li>相对于窄依赖，宽依赖付出的代价要高很多，尽量避免使用</li><li>例如：groupByKey、reduceByKey、sortByKey</li></ul></li></ul>',15)]))}const d=a(s,[["render",n]]),h=JSON.parse('{"path":"/guide/etl/spark/spark-introduce.html","title":"Spark","lang":"zh-CN","frontmatter":{},"headers":[{"level":2,"title":"Spark 简介","slug":"spark-简介","link":"#spark-简介","children":[{"level":3,"title":"Spark 概念","slug":"spark-概念","link":"#spark-概念","children":[]},{"level":3,"title":"Spark 特点","slug":"spark-特点","link":"#spark-特点","children":[]}]},{"level":2,"title":"Spark 原理","slug":"spark-原理","link":"#spark-原理","children":[{"level":3,"title":"编程模型","slug":"编程模型","link":"#编程模型","children":[]}]}],"git":{"updatedTime":1744726067000,"contributors":[{"name":"alice","username":"alice","email":"921757697@qq.com","commits":1,"url":"https://github.com/alice"}],"changelog":[{"hash":"d075898e68e7a6d129496b2eb84d6f43099ce89c","time":1744726067000,"email":"921757697@qq.com","author":"alice","message":"deploy"}]},"filePathRelative":"guide/etl/spark/spark-introduce.md"}');export{d as comp,h as data};
