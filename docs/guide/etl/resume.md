
2016年毕业后，一直从事大数据平台开发相关工作，从0到1打造数据集成产品，支持多种异构数据源间的海量数据同步，并提供离线、实时、全量、增量场景下全域数据集成解决方案，
目前支撑了数据集成需求。

技术要求：

1. 掌握Spark Core/SQL/Streaming，熟悉RDD/Dataset优化、Shuffle机制、内存管理；
2. 熟悉Flink DataStream/Table API、状态管理、Exactly-Once语义，有Flink SQL调优经验；
3. 了解Hadoop生态，熟悉HDFS、HBase、Hive、Kafka、Zookeeper等组件；
4. 了解数据仓库建模、ETL流程、数据质量保证、数据治理、数据可视化等相关知识；
5. 了解数据分析、机器学习、深度学习、推荐系统等领域的理论和技术；
6. 了解数据结构、算法、设计模式、数据库原理、计算机网络等基础知识。

